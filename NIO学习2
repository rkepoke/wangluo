基本原理

服务端打开一个通道（ServerSocketChannel），并向通道中注册一个选择器（Selector），这个选择器是与一些感兴趣的操作的标识（SelectionKey，即通过这个标识可以定位到具体的操作，从而进行响应的处理）相关联的，然后基于选择器（Selector）轮询通道（ServerSocketChannel）上注册的事件，并进行相应的处理。
客户端在请求与服务端通信时，也可以向服务器端一样注册（比服务端少了一个SelectionKey.OP_ACCEPT操作集合），并通过轮询来处理指定的事件，而不必阻塞。


///服务端,客户端代码稍后补上
实现原理

其实Java的NIO使用了IO多路复用，，I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。(这句话是重点,通过监听描述符,一旦那个描述符准备就绪,就通知程序执行相应的操作,一般是通过回调函数执行)
目前支持的IO多路复用有select,poll和epoll。
与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。(NIO的最大优势,不会造成线程资源的浪费,BIO会造成)

1.select
select本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理。这样所带来的缺点是：
    1.select最大的缺陷就是单个进程所打开的FD是有一定限制的(原来是单进程能打开的文件描述符是有限的,原因在这)，它由FD_SETSIZE设置，默认值是1024。
    　　一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048.
    2.对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低。(//第一:它轮询的是每一个IO连接,即套接字,每次select都要轮询全部的socket,不管这个socket数据有没有准备好,那么这浪费的其实是CPU时间.造成了效率低下和资源浪费的原因?那么如何解决呢?给每个socket注册一个回调函数,但满触发条件时,回调函数自动完成相关操作,这样就避免了轮询,即epoll
第二:其实select方式效率低下的第二个原因是:每次轮询是怎么完成的?就是调用系统函数去问的,而调用这个系统函数才是造成资源浪费的根本)
    　　当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度，不管哪个Socket是活跃的，都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。
    3.需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。

poll

基本原理：poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。
它没有最大连接数的限制，原因是它是基于链表来存储的，但是同样有一个缺点：
    1.大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。
    2.poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。


epoll        (epoll在内核中有张事件表,里面存放用户关系的文件描述符事件
		1:epoll支持两种触发机制,ET(edge-trigger)边缘触发和LE(level-trigger)水平触发,特点在与边缘触发,他会告诉进程那些fd文件描述符刚刚变成了就绪状态,回到上面所说,epoll是在epoll_ctl中注册fd,一旦某个fd准备就绪,内核就会采用callback的回调机制来激活这个就绪了的fd(每个fd都对应这一个具体的socket),然后epoll_wait就会收到通知去处理了
		2.epoll比select/poll效率高的原因就是,它不采用轮询,只有就绪的fd,会被callback激活处理,所以不会有不必要的轮训,可以理解为epoll指询问事件表中的fd,与总连接数无关)
epoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。
基本原理：epoll支持水平触发和边缘触发，最大的特点在于边缘触发，它只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。
epoll的优点：
    1.没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）。
    2.效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。
    　　只有活跃可用的FD才会调用callback函数；即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。
    3.内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。


epoll原理
    epoll是Linux下的一种IO多路复用技术，可以非常高效的处理数以百万计的socket句柄。
    c封装后的3个epoll系统调用

    int epoll_create(int size)  //创建epoll
    epoll_create建立一个epoll对象。参数size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。
    *nt epoll_ctl(int epfd, int op, int fd, struct epoll_event event)  //让创建的epoll监控socket句柄
    epoll_ctl可以操作epoll_create创建的epoll，如将socket句柄加入到epoll中让其监控，或把epoll正在监控的某个socket句柄移出epoll。
    *int epoll_wait(int epfd, struct epoll_event events,int maxevents, int timeout)   //就绪的fd,通知epoll_wait去告诉用户进程去处理.
    epoll_wait在调用时，在给定的timeout时间内，所监控的句柄中有事件发生时，就返回用户态的进程。

大概看看epoll内部是怎么实现的：
////重点啊,最重点.epoll_create,就file创建了一颗红黑树,和一个就绪队列,list
///epoll_ctl:会把socket放入红黑树上,在内核的中断处理程序中注册这个socket的回调函数,一旦这个socket的就绪了,就会触发中断,内核就会把它放入就绪队列,
///epoll_wait:只处理就绪列表里的socket,没有就sleep

epoll初始化时，会向内核注册一个文件系统，用于存储被监控的句柄文件，调用epoll_create时，会在这个文件系统中创建一个file节点。同时epoll会开辟自己的内核高速缓存区，以红黑树的结构保存句柄，以支持快速的查找、插入、删除。还会再建立一个list链表，用于存储准备就绪的事件。
当执行epoll_ctl时，除了把socket句柄放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后，就把socket插入到就绪链表里。
当epoll_wait调用时，仅仅观察就绪链表里有没有数据，如果有数据就返回，否则就sleep，超时时立刻返回。
epoll的两种工作模式：

LT：level-trigger，水平触发模式，只要某个socket处于readable/writable状态，无论什么时候进行epoll_wait都会返回该socket。
ET：edge-trigger，边缘触发模式，只有某个socket从unreadable变为readable或从unwritable变为writable时，epoll_wait才会返回该socket。




参考:
链接：https://www.jianshu.com/p/17dfe6a86214

